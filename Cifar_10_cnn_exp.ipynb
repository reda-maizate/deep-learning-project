{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6737c3f4",
   "metadata": {},
   "source": [
    "**ConvNets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ec4b4",
   "metadata": {},
   "source": [
    "Le principe d'utilisation d'un ConvNets reste les données a multiples dimensions.\n",
    "Dans notre cas les données sont des images c-à-d des données à deux dimensions \n",
    "Le convnets est le réseau qui s'adapte le mieux aux images puisque celui-ci s'inspire du cortex visuel\n",
    "Comme tout réseau la topologie est importante \n",
    "On a pas chercher ici à prendre le meilleur réseau puisque l'idée d'un réseau est de l'optimiser et le but est de comprendre comment les hyperparamètres peuvent changer un réseau et ses résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580acd1",
   "metadata": {},
   "source": [
    "Ici on à crée un réseau initiale et ce réseau va nous servir de base dans notre étude et en voici les paramètres : \n",
    "Les paramètres et le réseau choisis sont empirique et constitue une base d'étude "
   ]
  },
  {
   "cell_type": "raw",
   "id": "436b1ade",
   "metadata": {},
   "source": [
    "NUM_LAYERS = list(range(3, 6))\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 300\n",
    "SHUFFLE = True\n",
    "learning_rates = 0.001\n",
    "momentums = 0.9\n",
    "\n",
    "def ConvNet(num_layer, filters_by_layers):\n",
    "    input_layer = Input(shape=(32, 32, 3))\n",
    "    hidden_layers = input_layer\n",
    "    \n",
    "    for n in range(num_layer):\n",
    "        hidden_layers = Conv2D(filters_by_layers[n], (3, 3), padding=\"same\", activation=tanh)(hidden_layers)\n",
    "        hidden_layers = MaxPool2D()(hidden_layers)\n",
    "    \n",
    "    hidden_layers = Flatten()(hidden_layers)\n",
    "    output_layer = Dense(NUM_CLASSES, activation=softmax)(hidden_layers)\n",
    "    return Model(input_layer, output_layer)\n",
    "\n",
    "\n",
    "\n",
    "num_layer = int(np.random.choice(NUM_LAYERS, 1))\n",
    "filters_by_layers = [32*x for x in range(1, num_layer+1)]\n",
    "\n",
    "convnet = ConvNet(num_layer, filters_by_layers)\n",
    "convnet.compile(loss=categorical_crossentropy,\n",
    "            optimizer=SGD(lr, momentum=mom),\n",
    "            metrics=categorical_accuracy)\n",
    "CONVNET_LOG = os.path.join(LOG_DIR, \"convnet\",\n",
    "                f\"convnet_ep_{EPOCHS}_bs_{BATCH_SIZE}_opt_SGD_lr_{lr}_mom_{mom}_layers_{'_'.join(str(e) for e in filters_by_layers)}_af_tanh\")\n",
    "CONVNET_MODEL = os.path.join(MODEL_DIR, \"convnet\",\n",
    "                f\"convnet_ep_{EPOCHS}_bs_{BATCH_SIZE}_opt_SGD_lr_{lr}_mom_{mom}_layers_{'_'.join(str(e) for e in filters_by_layers)}_af_tanh.keras\")\n",
    "convnet.fit(x_train,\n",
    "            y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=(x_test, y_test),\n",
    "            shuffle=SHUFFLE,\n",
    "            callbacks=[tf.keras.callbacks.TensorBoard(CONVNET_LOG, histogram_freq=1)] \n",
    "           ) \n",
    "convnet.save(CONVNET_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68917b",
   "metadata": {},
   "source": [
    "# Ajouter image réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670a77e",
   "metadata": {},
   "source": [
    "## Réseau initiale "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a0ab7e",
   "metadata": {},
   "source": [
    "Les courbes ci dessus montre l'apprentissage (accuracy et loss) du réseau de base qui va nous servir de comparatif.\n",
    "Les courbes sont lisses sans dents de scie et une craoissance progressive sans décroissance.\n",
    "On peut observer entre l'epochs 150-200 du surapprentissage sur les courbes d'accuracy et de loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4de1be",
   "metadata": {},
   "source": [
    "## Max pooling vs Average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c75cc",
   "metadata": {},
   "source": [
    "Nous allons comparer deux type de couche utilisés dans les convnet : le max pooling et l'average pooling\n",
    "la courbes du dessus est celle du max pooling et celle d'en dessous est celle de l'avg pooling\n",
    "\n",
    "Lorsque l'on regarde les deux courbes on remarque qu'elle on des similitudes : une courbe lisse et stable sans dents de scie, un croissance progressive sans chute \n",
    "Néanmoins des différences sont visibles : la courbe du réseau initiale présente un croissance plus forte avec une différence de ~10% d'accuracy supplémentaire mais un surapprentissage notable à partir de la stabilisation du plateau. \n",
    "Lorsque l'Average pooling est utilisé, la croissance y est moins forte mais elle ne présente pas de surapprentissage.\n",
    "On peut en conclure une différence de performance entre les deux couches.\n",
    "\n",
    "\n",
    "> Comment peut on expliquer cette différence : que se passe t-il dans les couches ?\n",
    "Max pooling prend la valeur maximum de la portion d'image couverte par le kernel et Avg pooling sa valeur moyenne. La différence se tient ici. Etant donne que les valeurs prisent ne sont pas les mêmes alors la capacité à capturer l'information va changer aussi. Les caractéristiques des deux layers vont drastiquement influer sur les résultats. Il est possible que l'utilisation d'un maxpooling peut provoquer un manque de généralisation dans la compréhension des données et du problème. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b1b4b1",
   "metadata": {},
   "source": [
    "## Séquential vs Fonctionnal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd07ffc",
   "metadata": {},
   "source": [
    "Nous allons comparer ici deux réseaux qui utilise les mêmes caractéristiques avec fonction d'activation optimizer learning rate et momentum identique, seul la topologie va changer. Nous avons un réseau séquentiel et un réseau fonctionnel. \n",
    "\n",
    "Lorsque l'on regarde l'aprentissage des deux réseaux on remarque que les deux courbes sont similaires dans leur stabilité mais aussi dans leur surapprentissage. Les deux réseau présentes exactement les mêmes progressions.\n",
    "On peut en conclure que la topologie d'un réseau influe grandement \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7112dcc0",
   "metadata": {},
   "source": [
    "## Padding = same vs Padding = valid "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e529d5",
   "metadata": {},
   "source": [
    "Nous allons comparer deux versions d'un même paramètre qu'est le padding avec le padding en \"same\" ou en \"valid\". \n",
    "Lorsque l'on regarde l'apprentissage des deux réseaux on remarque que les deux courbes sont similaires dans leur stabilité et dans leur croissance avec un faible écart entre les courbes entre les epochs 50-150 puis qui s'accentue vers la fin. La différence que l'on peut regarder est que le réseau avec un padding = \"valid\" ne présente pas une fort surapprentissage mais l'accuracy y est plus faible. Les courbes de loss on pour la partie un nombre similaire avec entre 0.8-1\n",
    "\n",
    "\n",
    "> Comment peut on expliquer cette différence : que se passe t-il avec le padding ? Le principe du padding est dans notre cas de compenser le manque d'information présent sur les bords d'une image lorsque le kernel passe. Dans le cas ou le padding est en \"valid\" l'image est conserver telle qu'initialement donné au model. Cela implique que si le kernel arrive en bord d'image, il est possible d'avoir une perte d'information du à la dimension du kernel et de l'image. Dans l'autre donc \"same\", des valeurs vont être ajouter de part et d'autre de l'image pour conserver les bords et donc conserver de l'information. Cela peut se remarquer car pour deux réseaux similaires, en début de courbe on peut voir que l'accuracy ne croit pas autant entres les deux versions. Cette perte d'information permet au réseau ayant le paramètre padding=\"valid\" d'avoir une meilleur généralisation et donc moins de surapprentissage puisque celui n'obtient pas l'ensemble de l'information de l'image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8adbca6",
   "metadata": {},
   "source": [
    "## Fonction d'activation tanh relu sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5240436",
   "metadata": {},
   "source": [
    "Nous allons comparer 3 fonctions d'activations que sont tanh relu et sigmoid sur l'ensemble du réseau. \n",
    "Les courbes des différentes fonction d'activations sont inscrites sur l'image pour plus de clarté \n",
    "\n",
    "Deux profils de courbe peuvent être observer sur l'image : \n",
    "- Le premier est celui de la fonction sigmoid qui présente une croissance faible et quelques dents de scie mais sans surapprentissage \n",
    "- Le deuxième est celui de tanh et de relu avec une croissance forte (plus forte pour ReLu) relativement stable mais avec un aussi fort surapprentissage pour les deux fonctions. \n",
    "\n",
    "Les courbes de loss présentes les mêmes types de profils : \n",
    "\n",
    "- Sigmoid avec une décroissance faible mais stable \n",
    "- Tanh et ReLu en dent de scie et une nette croissance pour ReLu qui croit après l'epochs 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a1a908",
   "metadata": {},
   "source": [
    "## Optimizer SGD ADAM RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe493e1",
   "metadata": {},
   "source": [
    "Nous allons comparer 3 optimizer que sont SGD Adam et RMSprop \n",
    "On peut observer 3 profils de courbes différents avec : \n",
    "- RMSprop qui montre un manque de stabilité avec une progression en dent de scie (ici ne s'affiche pas mais en zoom oui) et un manque de croissance en accuracy et de décroissance en loss\n",
    "- Adam qui montre une croissance très forte et monte très vite sur les 100% d'accuracy mais présente toute aussi rapidement un fort surapprentissage avant même de dépasser les 50 epochs. On peut aussi observer une chute sous forme de pic vers l'epoch 350 en accuracy sur la validation et sur le train. La courbe de loss quant à elle présente les deux courbes validation et train en fin de run avec un valeur de 0 de loss pour le train et de 2.8 en validation a son maximum avec de respectivement augmenter et chuter pour former un pic.\n",
    "- SGD quant à lui montre un profil de base avec courbe liss et stable surapprentissage vers la fin sans pics croissance ou décroissance sur l'accuracy ou la loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f15043",
   "metadata": {},
   "source": [
    "## Learning Rate et momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd5f74",
   "metadata": {},
   "source": [
    "Nous allons ici étudier l'impact du learning rate sur l'apprentissage ainsi que le momentum. \n",
    "On peut diviser cela en plusieurs parties :  [0.5, 0.1, 0.05,0 .01, 0.005, 0.001] pour le learning rate et [0.25, 0.5, 0.9] pour le momentum\n",
    "- Pour 0.5 de learning rate et [0.25, 0.5, 0.9] de momentum: \n",
    "> L'accuracy ne dépasse pas les 10% avec une progression en dents de scie pour l'accuracy sur la courbe de de train et une stagnation sur la partie validation. La loss est elle aussi dans le même cas \n",
    "- Pour 0.1 de learning rate et [0.25, 0.5, 0.9] de momentum:  \n",
    "> Deux profils peuvent être vu : le premier avec des croissances allant jusqu'a 100% d'accuracy et un profil qui ne dépasse pas les 10%. Les deux profils similaires sont vu lorsque le momentum est à 0.25 et 0.5. On peut observer que sur les courbes un fort surapprentissage, une progression stable et un fort pic avant l'epoch 50 qui diminue drastiquement l'accuracy mais qui croit de nouveau est observé. Cette tendance est en miroir avec la loss qui elle aussi présente le même type de pic : forte décroissance, pic de croissance, et redécroissance\n",
    "Le profil avec l'accuracy ne dépassant pas les 10% est lorsque le momentum est à 0.9. L'accuracy décroit, est en dents de scie mais sans surapprentissage \n",
    "- Pour 0.05 de learning rate et [0.25, 0.5, 0.9] de momentum:  \n",
    "> Les trois courbes présentes des profils similiares : accuracy forte avec 100% atteint avant l'epoch 80 sur la courbe de train, courbe stable et un fort surapprentissage. Les courbes de validation ne dépassent pas les 75% d'accuracy. \n",
    "La loss présente pour les trois courbes aussi des profils similaires avec une forte baisse sur la courbe de validation. Un pic plus prononcé est visible sur la courbe au learning de 0.05 et de momentum 0.9, alors que pour les deux autres il s'agirait plutôt d'un puit mais qui dans les trois cas remontent. \n",
    "On peut remarquer que sur la courbe de loss de train que la décroissant est le même dans sa tendance mais pas dans sa progression. Plus le momentum est fort plus la courbe à tendance à fortmenent diminuer puis rejoint un plateau beaucoup pluôt que pour des valeurs faibles. \n",
    "- Pour 0.01 de learning rate et [0.25, 0.5, 0.9] de momentum:  \n",
    "> Cette fois ci on remarque concernant la partie accuracy que les courbes sont plus tassés. Sur la courbe de train celle ci sont stable sans dents de scie avec un convergence plus rapide pour un lr de 0.01 et momentum 0.9. La croissance de la courbe avec lr 0.01 et momentum 0.25 est moins fort que pour lr 0.01 et momentum 0.5 mais celle ci finit pas la dépasser. Les trois profils présentes un surapprentissage. Les courbes de validation sont moins stable et ne dépasse pas les 75% d'accuracy mais contrairement au train, elles convergent tous au même point c-à-d 75% d'accuracy. \n",
    "Les courbes de loss sont miroir en train par rapport au courbe de l'accuracy mais pas en validation :  une décroissance sous forme de puit est visible puis une forte croissance avec un lr de 0.01 et momentum de 0.9. Cette tendance se fait voir aussi avec un lr de 0.01 et un momentum de 0.25 qui recroit en fin d'apprentissage. Seul la courbe au lr de 0.01 et de momentum de 0.5 décroit constamment. \n",
    "- Pour 0.005 de learning rate et [0.25, 0.5, 0.9] de momentum: \n",
    "> Ici on peut observer deux profils : pour un lr de 0.005 et un momentum de 0.25 et 0.5 la croissance de la courbe est plus lent progressif en train et en validation avec un faible signe de surapprentissage. L'accuracy ne dépasse pas les 80%. La loss est miroir à ce profil. \n",
    "Lorsque le lr est de 0.005 et le momentum est de 0.9, l'accuracy atteint les 100% mais au détriment d'un fort surapprentissage. Les tendances y sont identiques avec un apprentissage similaire en vitesse mais pas en accuracy. En terme de loss le profil y est miroir à l'accuracy : décroissance forte avec cette fois ci un faible regain en fin d'apprentissage.\n",
    "- Pour 0.001 de learning rate et [0.25, 0.5, 0.9] de momentum: \n",
    "> Pour finir on peut observer que les profils de courbes sont relativement les mêmes en ecartant leur accuracy : croissance faible mais progressive avec un plateau moins prononcé en validation et en train. En loss cette tendance est identique. Néanmoins on peut y observer des différences et la première est que pour un lr de 0.001 et un momentum de 0.9 on y voit du surapprentissage et une courbe plus arrondi comparé aux deux autres réseaux. Ces réseaux ne sont pas en surapprentissage mais on une accuracy plus faible aux alentours de 50% en train comparé au 100% en train et 60% en validation du paramètrage lr de 0.001 et un momentum de 0.9 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
